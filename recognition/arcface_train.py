import pandas as pd
import numpy as np
import cv2
import sys

from sklearn.preprocessing import OneHotEncoder

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model, layers

from violinmatch.utils.config import make_secrets
from violinmatch.recognition.ArcFace_utils import L2Normalization, ArcLayer, ArcLoss, ValidationMetricsCallback, TrainDataGenerator, load_violin_cropped



def make_model(n_classes : int, fine_tune_layers : int = 0, input_size = 380, square : bool = False):
    """
    returns a 1 neuron layer, with softmax or sigmoid
    efficientnetB0 input size: 224
    efficientnetB4 input size: 380
    """
    if square:
        backbone = EfficientNetV2S(include_top=False, input_shape = (input_size, input_size, 3))
    else:
        backbone = EfficientNetV2S(include_top=False, input_shape = (int(input_size*2), int(input_size/2), 3))
    backbone.trainable = False
    if fine_tune_layers:
        for layer in backbone.layers[-fine_tune_layers:]:
            if not isinstance(layer, layers.BatchNormalization):
                layer.trainable = True

    flatten = layers.Flatten()(backbone.output) 
    embedding = layers.BatchNormalization()(flatten)
    
    embedding = layers.Dense(512)(embedding)
    embedding = layers.BatchNormalization()(embedding)
    embedding = L2Normalization(name = "embedding")(embedding)
    output = ArcLayer(n_classes)(embedding)
    return Model(backbone.input, output, name = "classifier")


def funct(property_id_list):
    """
    map function to be used in a tensorflow input pipeline.
    input: a list of property_id, generated by the from_tensor_slices method
    output: a tensor containing preprocessed and augmented images and a list of corresponding labels
    """
    global train_df, df_kp, secrets, input_size, one_hot_encoder, hard_ID_list

    # n hard property_IDs (previously identified), are randomly added to the property_ID list
    n=30
    hpid = hard_ID_list[~np.isin(hard_ID_list, property_id_list.numpy())]
    np.random.shuffle(hpid)
    pid_list = np.concatenate((property_id_list.numpy(), hpid[:n]))

    tt = train_df.loc[train_df.property_id.isin(pid_list), ["property_id", "instance_id"]]
    instance_id_list = tt.instance_id.values
    labels = one_hot_encoder.transform(np.expand_dims(tt.property_id.values,1)).toarray()
    labels = tf.constant(labels, dtype = tf.float32)

    images = load_violin_cropped(
        instance_id_list = instance_id_list,
        keypoints_df = df_kp, 
        secrets = secrets, 
        size = input_size, 
        square = False, 
        augmentation = True,
        resize = 5e5)
    
    images = tf.constant(images, tf.uint8)
    
    return images, labels



if __name__ == '__main__':

    

    input_size = 380
    batch_size = 75
    max_epoch = 21
    ckpt_path = 'data/arcface_df_2/ckpt/V2S_Final'
    secrets = make_secrets()

    old_train_df = pd.read_csv('data/arcface_df_2/train_df.csv', index_col=0) # got rid of some violins (bad ones), still need the old dataframe for the one hot encoding
    train_df = pd.read_csv('data/arcface_df_2/new_train_df.csv', index_col=0)
    val_df = pd.read_csv('data/arcface_df_2/val_df.csv', index_col=0)
    df_kp = pd.read_csv('data/arcface_df_1/df_kp.csv', index_col=0)

    # to detect violin fronts incorrectly tagged as violin backs by the keypoint model, we add known fronts in the arcface classifier. 
    # The trained model will then classify violin fronts in the "front" class.

    front_df = pd.read_csv('data/arcface_df_1/front_df.csv', index_col=0)
    front_df = front_df.sample(frac=1).iloc[:100].reset_index(drop=True)
    front_df['property_id'] = -1
    
    train_df = pd.concat((train_df, front_df), ignore_index=True)
    hard_ID_list = np.load('data/arcface_df_2/new_hard_ID.npy')
    print('train dataset: ', train_df.shape)
    print('validation dataset: ', val_df.shape)

    print('\n*** val dataset ***\n') 
    ## preprocess validation images only once. The resulting tensor is small enough to be used throughout the training process.

    # val_images = load_violin_cropped(
    #     instance_id_list = val_df.instance_id.values,
    #     keypoints_df = df_kp,
    #     secrets = secrets,
    #     size = 380,
    #     square = False,
    #     augmentation = False,
    #     loading_bar=True,
    #     resize = None
    # )

    ## save/load validation images

    # np.save('data/arcface_df_2/val_imgs.npy', val_images)
    val_images = np.load('data/arcface_df_2/val_imgs.npy')

    print('\n*** train dataset ***\n') # fit one hot encoder and create train dataset

    one_hot_encoder = OneHotEncoder()
    old_train_df = pd.concat((old_train_df, front_df), ignore_index=True)
    train_ID = old_train_df.property_id.unique()
    one_hot_encoder.fit(np.expand_dims(train_ID,1))

    # using the following technique, the GPU/CPU use is optimized, except for the data loading/preprocessing/augmentation which are not vectorizable.
    train_ID = tf.constant(train_ID, dtype = tf.int32)
    train_dataset = tf.data.Dataset.from_tensor_slices(train_ID)
    train_dataset = train_dataset.shuffle(buffer_size=train_ID.shape[0])
    train_dataset = train_dataset.batch(batch_size)
    train_dataset = train_dataset.map( 
                        map_func = lambda i: tf.py_function(
                            func=funct,
                            inp=[i], 
                            Tout=[tf.uint8, tf.float32]), 
                        num_parallel_calls=tf.data.AUTOTUNE
                        )
    train_dataset = train_dataset.prefetch(buffer_size = tf.data.AUTOTUNE)


    print(f'\n*** make model input shape: {train_ID.shape[0]} ***\n')
    model = make_model(fine_tune_layers=5, input_size=input_size, n_classes = train_ID.shape[0])
    model.load_weights("data/arcface_df_2/ckpt/V2S_5")


    print('\n*** compile model ***\n')
    model.compile(
        loss =  ArcLoss(margin=0.5),
        optimizer=Adam(learning_rate = 1e-4), 
        )

    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_path,
                                                 save_weights_only=True,
                                                 #save_best_only=True,
                                                 verbose=1)

    def scheduler(epoch, lr):
        """
        learning rate scheduler
        """
        if epoch == 0:
            lr=5e-4
        if epoch >0:
            lr *= 0.5
        if (epoch+1)%7 == 0:
            lr=1e-3
        print("****** scheduler ****** ", lr)
        return lr
    schedule_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)


    import wandb
    from wandb.keras import WandbCallback
    print('\n***  init wandb ***\n')

    wandb.init(project="arcface", entity="padda")

    print("*** begin training ***\n")

    model.fit(\
        train_dataset, \
        epochs = max_epoch, \
        callbacks = [\
            schedule_callback,
            cp_callback,
            ValidationMetricsCallback(val_images, val_df.property_id.values, n_step=900),
            WandbCallback(log_batch_frequency = 4, save_model=False)
            ]
        )
